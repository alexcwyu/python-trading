For some reason, the close series of HSI before the pandas_db feed load it, so when we load there we have a NAN series

### Dataframe / Series
- to and from pandas
- to and from Protobuf

there is a bug in dataframe. When bar has a attribute that no inside the dataframe load, and when we append,
in the rc's dataframe we supposed to add a new column. But the "series_dict" part is now missing.
So we have to refactor the add column in "to_series_dict" and let append_row to call add column when the "col" is not in the
series_dict's key

- persist to and from DB
- serialize and deserialize via network



- register to globalcontext


- offline (batch) vs online (realtime update) mode
    - DAG (batch mode, pull from child, recursively up to the root)
        - parent
            store lasttimestamp
            return value since timestamp (can be empty array)

        - child
            store parentId
            store last update timestamp & last result (conflated into a dict) from each parent
            store last updatetimestamp
            batch calculate and store the result.

            when compute / evaluate, check lastcomputed timestamp

        Question:
        - when we load the child series by get_series, do we check if parent series has benn loaded to memory? if not, create it?


    - subscribe (realtime update, push from parent)
        - rxpy
        - publish
        - when subscriber receive update, update the parent timestamp


- DataFrame
    - It will serve as a derived object during the algo app lifetime, it will never serialized back as Dataframe in database.
    - Only Series is stored in DB
    - So every dataframe will breakdown into series and save into the database
    - Reload of the dataframe from dataframe will be done by first retrieve back the series, and construct it to dataframe
    - We are bounded by Google Protobuf that it can't store 2D array. Or course we can always enumerate 2D array by properly indexed 1D array
    - Raccoon Dataframe will still be used since if we want to use the Rx's combine latest, it will modify come column series,
        so it is better to use the Raccoon Dataframe as mirror or view while the original series


- multi-index



DataFrame
    - holding multiple series


EvaluationContext (global)
    def evaluate():
        - for each input:
            input[self.last_ts:] -> timeseries
            put the ts into a dict

        if the dict of timeseries is empty:
            no further update.

        if the dict of timeseries is non empty:
            merge into a DF
            iterate the DF (backfill for na?)
            call the update method

        acutally....
            covert to pandas in global context
            just use pandas to calculate the value
            convert back to series

Expression
    def evaluate():
        create a EvaluationContext
        slice for each parent since last calculate into a series, add to EvaluationContext
        EvaluationContext.evaluate()
        call the compute()


    def on_event(timestamp: long, input: str, data: Dict):


    def compute(timestamp: long, input_data: Dict[str, double]):


    def publish(timestamp, output_data:Dict[str, double]):



    - slice for each parebnt

def update(input, ):


#########


# take care the conversion between datetime and unixtime.
#https://stackoverflow.com/questions/15203623/convert-pandas-datetimeindex-to-unix-time

the following does not match!

import pandas as pd
Backend TkAgg is interactive backend. Turning interactive mode on.
df = pd.DataFrame({"ts" : [now], "a" : [20]})
pd.to_datetime(df['ts'])
Out[10]:
0   2017-08-31 00:36:22.876549
Name: ts, dtype: datetime64[ns]
df['ts'] = pd.to_datetime(df['ts'])
df
Out[12]:
    a                         ts
0  20 2017-08-31 00:36:22.876549
df['ts'].values
Out[13]: array(['2017-08-31T00:36:22.876549000'], dtype='datetime64[ns]')
df['ts']
Out[14]:
0   2017-08-31 00:36:22.876549
Name: ts, dtype: datetime64[ns]
df = df.set_index('ts')
df
Out[16]:
                             a
ts
2017-08-31 00:36:22.876549  20
df.index[0]
Out[17]: Timestamp('2017-08-31 00:36:22.876549')
df.index[0].value
Out[18]: 1504139782876549000
df.index[0].value // 10**9
Out[19]: 1504139782
df.index[0].value // 10**9 - date_to_unixtimemillis(now)
Out[20]: -1502604660218
df.index[0].value // 10**9 - date_to_unixtimemillis(now)
Out[21]: -1502604660218
df.index[0].value // 10**9
Out[22]: 1504139782
date_to_unixtimemillis(now)
Out[23]: 1504108800000



